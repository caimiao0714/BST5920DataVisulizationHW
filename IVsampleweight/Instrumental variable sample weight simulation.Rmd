---
title: "Instrumental variable sample weight simulation"
author: "Miao Cai"
date: "12/21/2018"
output:
  html_document:
    theme: yeti
    toc: true
    toc_float: true
    number_sections: true
    df_print: paged
    code_folding: hide
  pdf_document:
    number_sections: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Instrumental variable

Consider a simple regression model:

$$Y = \beta_0 + \beta_1 Tr + \epsilon$$

An instrumental variable $IV$ is correlated with $Tr$ but uncorrelated with the outcome$Y$except through $X$.

# An overview

Suppose that you have a continuous variable \( y \) with the known mean
response function

\[ E(y) = \beta_0 + \beta_1 x + \beta_2 c \]

and further that $x$ and $c$ are correlated with each other. If you knew $c$,
estimating $\beta_1$ would be easy by simply running `lm(y ~ x + c)`. You
would complete the regression, throw together a few diagnostics and call it a
day.

But often we don't observe $c$ in our data. $c$ could be any number of things,
such as treatment practices at a hospital or unmeasured differences between
patients, but it is in the direct casual path of $y$ and you don't know it. If
you instead do the regression `lm(y ~ x)`, you will not get an accurate
estimate of $\beta_1$. If you think about the model fit in that call it looks
like

\[ y_i = \beta_0 + \beta_1 x_i + \epsilon_i \]
but in this case
\[ \epsilon_i = f(c_i, \eta_i) \]

where $\eta_i$ is white noise centered on zero. As long as that function is
linear, the expectation of $\epsilon$ will be some multiple of $c$. The result
is that $x$ and $\epsilon_i$ are correlated. The estimates of $\beta_1$ will
nessecarly be wrong. Interested parties can read more [here](https://en.wikipedia.org/wiki/Instrumental_variables_estimation#Estimation)
and see just how wrong it will be.

Suppose there there is a third variable $z$, $x$ can be expressed as
some function of $z$, $z$ has no effect on $y$ except through $x$ (therefore $z$ 
is independent of any other variable that effects $y$). Then we have the
equations


\[ E(x) = \alpha_0 + \alpha_1 x^* + \alpha_2 z \]
and
\[ E(y) = \beta_0 + \beta_1 x + f(c) \]

where $x^*$ is some latent part of $x$ and $c$ is still unobserved. Looking at
the first equation and realizing we don't know what $x^*$ is the resulting model
will have two regression coefficients. The intercept will be
$\alpha_0 + \alpha_1 E(x^*)$ which is not correlated with $f(c)$ as
and $z$ is independent of $f(c)$ by assumption.

If we take the fitted values of $x$ from the first equation and plug it into the
second equation, the $x$ term is no independent of $f(c)$. This independence
means that we can produce consistent estimates of$\beta_1$when we replace $x$ 
with the fitted values of $x$ from equation one. When the estimator used is OLS,
this method is called two-stage least squares (2SLS).